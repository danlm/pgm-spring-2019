---
layout: distill
title: Introduction to Graphical Models
description: an example of a distill-style lecture notes that showcases the main elements
date: 2019-01-14

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Daniel Martin  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Yiwen Yuan
    url: "#"
  - name: Youngseog Chung
    url: "#"
  - name: Siddharth Satpathy
    url: "#"

editors:
  - name: Maruan Al-Shedivat  # editor's full name
    url: "https://www.cs.cmu.edu/~mshediva/"  # optional URL to the editor's homepage

abstract: >
  In this first lecture, we give an introduction to graphical models. 
We begin by discussing the three fundamental question types that graphical models can help us answer: representation, inference, and learning. 
We then show how graphical models can incorporate domain knowledge, fuse heterogeneous data, and support Bayesian inferences. 
Subsequently, we define the two types of graphical models (directed and undirected) and show that model graph traversal can yield independence relationships (the Equivalence Theorem). 
We conclude with examples of fancier models as well as application areas.
---

***

## The Fundamental Questions of Graphical Modeling

A graphical model is a method of modeling a probability distribution for reasoning under uncertainty, which is needed in applications such as speech recognition and computer vision.
We usually have a sample of datapoints: $$ D = \{X_{1}^{(i)},X_{2}^{(i)},...,X_{m}^{(i)} \}_{i=1}^N $$.
The relations of the components in each $$ X $$ can be depicted using a graph $$ G $$.
We then have a our model $$ M_G $$.

<insert graph image here>

Graphical models allow us to address three fundamental questions:

1. How should I represent my data in a way that reflects domain knowledge while acknowledging uncertainty?

2. How do I make inferences from this data?

3. How can I learn the `"right"` model for this data?

Each of these questions can be rephrased as a question about probability distributions:

1. What is the joint probability distribution over my input variables? 
Which state configurations of the distribution are actually relevant to the problem?

2. How can we obtain the state probabilities? 
Do we use maximum-likelihood estimation, or can we use domain knowledge?

3. How can we compute conditional distributions of unobserved (latent) variable without needing to sum over a large number of state configurations?

In the next section, we give an example to show how graphical models provide an effective way of answering these questions. 


***
## Structure within a Cell: real world example of graphical models with structure among the RVs

<insert graph image here>

Receptors: receives signal from cell surface
Kinase: Reads and decodes the signal
TF: Takes in the signal and triggers production of dna with dna template
Gene: DNA templates

We can incoporate such domain knowledge to impose structure on the RVs $$X_1,..,X_8$$.
A preliminary way is to partition the RV's into compartments they reside in within a cell.
Then we can model edges(pathway) that model the dependencies(communication) among the RVs(nodes).


With this structure, we can better express the joint probabilities among the RVs than with a full joint distribution table. The Factorization Law gives us a way to do so. 
The Factorization Law is a graph traversal algorithm that outputs a unique representation of the joint probability of the RVs. Concisely, we traverse the graph and identify the conditional probabilities of each node given its parent nodes, and marginal probabilities of nodes that do not have parents, then multiply all terms together for the joint probability of all nodes.

There are 3 main benefits of representing the joint distribution in this manner (with a graph structure and conditional probabilities that tie parent nodes and child nodes)

***

## 3 Main Benefits of Graphical Models

1. Save cost in representing the joint distribution
By modeling the dependencies among the RVs with a graph and conditionals, the number of parameters needed to decribe the joint distribution is much fewer than when using a full joint distribution table.

Assume all RVs are binary.

$$ P(X_1) \rightarrow 1 $$ parameter 
$$ P(X_2) \rightarrow 1 $$ parameter 
$$ P(X_3|X_1) \rightarrow 2 $$ parameters 
$$ P(X_4|X_2) \rightarrow 2 $$ parameters
$$ P(X_5|X_2) \rightarrow 2 $$ parameters
$$ P(X_6|X_3, X_4) \rightarrow 4 $$ parameters 
$$ P(X_7|X_6) \rightarrow 2 $$ parameters 
$$ P(X_8|X_5, X_6) \rightarrow 4 $$ parameters

Total = 18 parameters

Meanwhile, with a full joint distribution table, we would need $$2^8-1$$ parameters.

2. Data integration

By factoring the joint distribution into modular terms, each term becomes self-contained and we can estimate each term with only the relevant datapoints (e.g. to estimate $$P(X_8|X_5, X_6)$$ we only need data for $$X_8, X_5 and X_6$$)
Therefore, the problem of joint distribution estimcation can be modularized into smaller pieces and integrated later by multiplication.

Examples:
1) Regarding the cell molecules example, one lab can study the subtree formed by $$X_1, X_3, X_6, X_7, X_8$$ and another lab can study $$X_2, X_4, X_5$$, then fuse their estimations together by multiplying the terms by their dependencies.
2) For a cellphone user, we can separately study the distribution represented by the user's text, image and network data and fuse them toegether with a graphic model to derive the joint distribution.
3) In genomics/biology, we routinely combine various data together with graphic models.

3. Graphical models provide a generic method of representing knowledge and making inferences

We can encode our domain knowledge through priors and incorporate them into our inferece via the Bayes Theorem:
$$ p(h|d) = \frac{p(d|h)p(h)}{p(d)} $$
$$ p(h|d) $$=posterior
$$ p(d|h) $$=likelihood
$$ p(h) $$=prior

A graphical model provides a structured and efficient way for doing these computations.
Therefore, a graphical model along with the Bayes Theorem provide a universal way of representing knowledge and computation.

***

## PGM's vs GM's

Next, we will elaborate on the difference between Probabilistic Graphical Models (PGM) and Graphical Models (GM).
In brief, a PGM adds structure to a multivariate statistical distribution, while a GM adds structure to any multivariate objective function.

A PGM minimizes the cost of designing a probability distribution. 
Formally, a PGM is a family of distributions over a given set of random variables.
These distributions must be compatible with all the independence relationships among the variables, which are encoded in a graph.

In the graph itself, the type of edge used denotes the relationship among the variables.
Directed edges denote causality, while undirected edges denote correlation.

<insert graph images here>

For instance, the Bayes net uses a directed acyclic graph (DAG).
Each node in a Bayes net has a Markov blanket, composed of its parents, its children, and its children's parents.
Every node is conditionally independent of the nodes outside its Markov Blanket.
Therefore, the local conditional probabilities as well as the graph structure completely determine the joint probability distribution.
This model can be used to generate new data.

By contrast, the Markov Random Field uses an undirected graph.
Every node is conditionally independent of the other graph nodes, except for its immediate neighbors.
To determine the joint probability distribution, we need to know local contingency functions as well as structural cliques.
This model cannot explicitly generate new data.

***

## Equivalence Theorem / PGM Genealogy

We will be discussing about the Equavalence Theorem, which stats as follows:

For a graph G, 

Let $D_1$ denote the family of all distributions that satisfy I(G), 

Let $D_2$ denote the family of all distributions that factor according to G, 

then $$D_1 \equiv D_2$$.

The theorem is interpreted in two ways:

1. Separation properties in the graph imply independence properties about the associated variables.

2. For the graph to be useful, any conditional independence properties we can derive from the graph should hold for the probability distribution that the graph represents.

The study of Graphical Models involves the following parts:

1. Density estimation with parametric and nonparametric methods

2. Regression : Linear, conditional mixture, nonparametric

3. Classification with generative and discriminative approaches

4. Clustering

An example of genealogy of graphical models is as follows: 

<img src="{{ '/assets/img/notes/lecture-01/gmm.png' | relative_url }}" />
