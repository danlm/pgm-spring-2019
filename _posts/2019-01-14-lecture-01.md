---
layout: distill
title: Introduction to Graphical Models
description: an example of a distill-style lecture notes that showcases the main elements
date: 2019-01-14

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Daniel Martin  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Yiwen Yuan
    url: "#"
  - name: Youngseog Chung
    url: "#"
  - name: Siddharth Satpathy
    url: "https://siddsatpathy.wixsite.com/portfolio"

editors:
  - name: Maruan Al-Shedivat  # editor's full name
    url: "https://www.cs.cmu.edu/~mshediva/"  # optional URL to the editor's homepage

abstract: >
  In this first lecture, we give an introduction to graphical models. 
We begin by discussing the three fundamental question types that graphical models can help us answer: representation, inference, and learning. 
We then show how graphical models can incorporate domain knowledge, fuse heterogeneous data, and support Bayesian inferences. 
Subsequently, we define the two types of graphical models (directed and undirected) and show that model graph traversal can yield independence relationships (the Equivalence Theorem). 
We conclude with examples of fancier models as well as application areas.
---

## Equations

This theme supports rendering beautiful math in inline and display modes using [KaTeX](https://khan.github.io/KaTeX/) engine.
You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`.
If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph.
Here is an example:

$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$

Note that [KaTeX](https://khan.github.io/KaTeX/) is work in progress, so it does not support the full range of math expressions as, say, [MathJax](https://www.mathjax.org/).
Yet, it is [blazing fast](http://www.intmath.com/cg5/katex-mathjax-comparison.php).

***

## Citations

Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

***

## Footnotes

Just wrap the text you would like to show up in a footnote in a `<d-footnote>` tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>

***

## Code Blocks

Syntax highlighting is provided within `<d-code>` tags.
An example of inline code snippets: `<d-code language="html">let x = 10;</d-code>`.
For larger blocks of code, add a `block` attribute:

<d-code block language="javascript">
  var x = 25;
  function(x) {
    return x * x;
  }
</d-code>

***
## Class Logistics
This is the Spring 2019 edition of Probabilistic Graphical Models class (10-708) of Carnegie Mellon University. This is a graduate level course which aims to acquaint students with tools from probabilistic graphical models to solve problems in fields like machine learning, artificial intelligence, statistics, probability theory, computer vision, natural language processing and graph theory. 

Here, we will like to touch upon a few things related to course logistics. 

1. Class website
The class website can be found at https://sailinglab.github.io/pgm-spring-2019/ .

2. Books
Mentioned below are a couple of textbooks that are recommended for this course.


  2.1. Daphne Koller and Nir Friedman, Probabilistic Graphical Models:
This book discusses frameworks for constructing and using probabilistic models of complex systems in a variety of settings. It explores many models like Markov networks, Bayesian models, discrete and continuous models, and techniques like inference, and learning to analyze data. The book also examines the use of these probabilistic graphical models for decision making and reasoning. This is a required book for the class. One has to note that the material of this class may go beyond this book.

  2.2. M. I. Jordan, An Introduction to Probabilistic Graphical Models: 
This book presents graphical model as a marriage between probability theory and graph theory. It discusses essential topics like probabilistic inference, learning and independence. A few excerpts of this book will be used for the class, and copies of relevant chapters will be made available to students.

  2.3. Kevin Murphy, Machine Learning - A Probabilistic Perspective:
This is a book for beginning graduate students or upper level undergraduates. It gives a holistic introduction to the field of machine learning, based on a unified, probabilistic approach. It discusses topics like probability, optimization, conditional random fields, regularization, and deep learning. The use of this book for this course is optional.


3. Course staff


  3.1. Professor: Professor Eric Xing (GHC 8101, office hours: Monday 1:30-2:30pm)

  3.2. Class Assistant: Amy Protos (GHC 8001)

  3.3. Teaching Assistants (TAs): Maruan Al-Shedivat (GHC 8229), Lisa Lee (GHC 8011), Xun Zheng (GHC 8013), Hao Zhang (NSH 4225), Paul Liang (GHC 8011). More details of office hours of TAs can be found at https://sailinglab.github.io/pgm-spring-2019/ .

  3.4. Students can contact the course instructors at 10708-instructor@cs.cmu.edu , or use Piazza for their questions.


4. Grading
Grading in this course is split between a variety of tasks. The course has 4 homework assignment which account for 40 \% of grades. The final project is worth 46 \% of all points in the course. Each student also has to scribe 1-2 lectures in the course; this accounts for another 10 \% of the course grade. Participation in discussions on Piazza, completion of mid-semester evaluation and contribution to actions that improve the class are worth 3.5 \%, 0.5 \% and 1 \% of the course grade. There are no exams in this class.

One can visit https://sailinglab.github.io/pgm-spring-2019/logistics/#grading to know more about grading and policies for late submissions.

5. Projects
Students are expected to form groups of 3-4 for projects in this course. The project will have four main parts, viz. proposal, a midway report, a final report, and a poster (or oral) presentation. The projects would be expected to be based on research, and write-ups would be expected to be similar to conference style papers.



## Structure within a Cell: real world example of graphical models with structure among the RVs

<insert graph image here>

Receptors: receives signal from cell surface
Kinase: Reads and decodes the signal
TF: Takes in the signal and triggers production of dna with dna template
Gene: DNA templates

We can incoporate such domain knowledge to impose structure on the RVs $$X_1,..,X_8$$.
A preliminary way is to partition the RV's into compartments they reside in within a cell.
Then we can model edges(pathway) that model the dependencies(communication) among the RVs(nodes).


With this structure, we can better express the joint probabilities among the RVs than with a full joint distribution table. The Factorization Law gives us a way to do so. 
The Factorization Law is a graph traversal algorithm that outputs a unique representation of the joint probability of the RVs. Concisely, we traverse the graph and identify the conditional probabilities of each node given its parent nodes, and marginal probabilities of nodes that do not have parents, then multiply all terms together for the joint probability of all nodes.

There are 3 main benefits of representing the joint distribution in this manner (with a graph structure and conditional probabilities that tie parent nodes and child nodes)

***

## 3 Main Benefits of Graphical Models


1. Save cost in representing the joint distribution
By modeling the dependencies among the RVs with a graph and conditionals, the number of parameters needed to decribe the joint distribution is much fewer than when using a full joint distribution table.

Assume all RVs are binary.

$$P(X_1) \rightarrow 1$$ parameter 
$$P(X_2) \rightarrow 1$$ parameter 
$$P(X_3|X_1) \rightarrow 2$$ parameters 
$$P(X_4|X_2) \rightarrow 2$$ parameters
$$P(X_5|X_2) \rightarrow 2$$ parameters
$$P(X_6|X_3, X_4) \rightarrow 4$$ parameters 
$$P(X_7|X_6) \rightarrow 2$$ parameters 
$$P(X_8|X_5, X_6) \rightarrow 4$$ parameters

Total = 18 parameters

Meanwhile, with a full joint distribution table, we would need $$2^8-1$$ parameters.

2. Data integration

By factoring the joint distribution into modular terms, each term becomes self-contained and we can estimate each term with only the relevant datapoints (e.g. to estimate $$P(X_8|X_5, X_6)$$ we only need data for $$X_8, X_5 and X_6$$)
Therefore, the problem of joint distribution estimcation can be modularized into smaller pieces and integrated later by multiplication.

Examples:
1) Regarding the cell molecules example, one lab can study the subtree formed by $$X_1, X_3, X_6, X_7, X_8$$ and another lab can study $$X_2, X_4, X_5$$, then fuse their estimations together by multiplying the terms by their dependencies.
2) For a cellphone user, we can separately study the distribution represented by the user's text, image and network data and fuse them toegether with a graphic model to derive the joint distribution.
3) In genomics/biology, we routinely combine various data together with graphic models.

3. Graphical models provide a generic method of representing knowledge and making inferences

We can encode our domain knowledge through priors and incorporate them into our inferece via the Bayes Theorem:
$$ p(h|d) = \frac{p(d|h)p(h)}{p(d)} $$
$$p(h|d)$$=posterior
$$p(d|h)$$=likelihood
$$p(h)$$=prior

A graphical model provides a structured and efficient way for doing these computations.
Therefore, a graphical model along with the Bayes Theorem provide a universal way of representing knowledge and computation.


## Why should we study graphical models?

Design and analysis of algorithms in fields artificial intelligence, machine learning, natural language processing etc encounter issues like uncertainty and complexity. In graphical models, we use the idea of modularity, and view such complex problems as combinations of simpler parts. Tools from graphical models can be used for communication of information in networks. They can also be used to ease computation (simplify computational complexities and reduce time required for computations.) As such, graphical model formalism can be used for development of efficient software packages for decision making and learning in problems rely on huge datasets.

Below we mention a few prominent reasons why one can use probabilistic graphical models:

1. In graphical models we break tasks into combinations of simpler parts. Probability theory helps to connect these simple parts with each other in a coherent and consistent manner.

2. Graph theory gives an easy to understand interface in which models with multiple variables can be cast. Such interfaces help to uncover interactions, dependencies between difference sets of variables. As a consequence, graph theory also helps in the design of more efficient algorithms.

3. Formalisms in general graphical model can be used for tasks in a plethora of fields like information theory, cyber security, systems engineering, pattern recognition etc.

4. The generalizability of graphical model framework gives us a way to view different systems as occurences of a common underlying formalism. 


## Plans for class

In this course, we will see an in-depth exploration of issues related to learning within the probabilistic graphical model formalism. The course will be divided into three main sections: Fundamentals of graphical models, advanced topics in graphical models, popular graphical models and applications. An outline of the topics that will be covered in this class is given below:

1. Fundamentals of graphical models

  1.1 Bayesian Network and Markov Random Fields

  1.2 Discrete, Continuous and Hybrid models, Exponential family, Generalized Linear Models

  1.3 Inference and Learning



2. Advanced topics and latest developments in graphical models

  2.1 Approximate inference

  2.2 Infinite graphical models: nonparametric Bayesian models

  2.3 Optimization-theoretic formulations for graphical models, e.g., Structured sparsity

  2.4 Graphical models vs Deep nets

  2.5 Nonparametric and spectral graphical models

  2.6 Alternative graphical model learning paradigms



3. Popular graphical models and applications

  3.1 Multivariate gaussian models

  3.2 Conditional random fields

  3.3 Mixed-membership    

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

***

## Print

Finally, you can easily get a PDF or printed version of the notes by simply hitting `ctrl+P` (or `⌘+P` on macOS).
