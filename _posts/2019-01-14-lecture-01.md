---
layout: distill
title: Introduction to Graphical Models
description: Introducing why graphical models are useful, and an overview of the main types of graphical models.
date: 2019-01-14

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Daniel Martin  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Yiwen Yuan
    url: "#"
  - name: Youngseog Chung
    url: "#"
  - name: Siddharth Satpathy
    url: "https://siddsatpathy.wixsite.com/portfolio"

editors:
  - name: Maruan Al-Shedivat  # editor's full name
    url: "https://www.cs.cmu.edu/~mshediva/"  # optional URL to the editor's homepage

abstract: >
  In this first lecture, we give an introduction to graphical models. 
  We begin by discussing the three fundamental question types that graphical models can help us answer: representation, inference, and learning. 
  We then show how graphical models can incorporate domain knowledge, fuse heterogeneous data, and support Bayesian inferences. 
  Subsequently, we define the two types of graphical models (directed and undirected) and show that model graph traversal can yield independence relationships (the Equivalence Theorem). 
  We conclude with examples of fancier models as well as application areas.
---

<!––  ## Class Logistics
This is the Spring 2019 edition of Probabilistic Graphical Models class (10-708) of Carnegie Mellon University. This is a graduate level course which aims to acquaint students with tools from probabilistic graphical models to solve problems in fields like machine learning, artificial intelligence, statistics, probability theory, computer vision, natural language processing and graph theory. 

Here, we will like to touch upon a few things related to course logistics. 

1. Class website
The class website can be found at https://sailinglab.github.io/pgm-spring-2019/ .

2. Books
Mentioned below are a couple of textbooks that are recommended for this course.


  2.1. Daphne Koller and Nir Friedman, Probabilistic Graphical Models:
This book discusses frameworks for constructing and using probabilistic models of complex systems in a variety of settings. It explores many models like Markov networks, Bayesian models, discrete and continuous models, and techniques like inference, and learning to analyze data. The book also examines the use of these probabilistic graphical models for decision making and reasoning. This is a required book for the class. One has to note that the material of this class may go beyond this book.

  2.2. M. I. Jordan, An Introduction to Probabilistic Graphical Models: 
This book presents graphical model as a marriage between probability theory and graph theory. It discusses essential topics like probabilistic inference, learning and independence. A few excerpts of this book will be used for the class, and copies of relevant chapters will be made available to students.

  2.3. Kevin Murphy, Machine Learning - A Probabilistic Perspective:
This is a book for beginning graduate students or upper level undergraduates. It gives a holistic introduction to the field of machine learning, based on a unified, probabilistic approach. It discusses topics like probability, optimization, conditional random fields, regularization, and deep learning. The use of this book for this course is optional.


3. Course staff


  3.1. Professor: Professor Eric Xing (GHC 8101, office hours: Monday 1:30-2:30pm)

  3.2. Class Assistant: Amy Protos (GHC 8001)

  3.3. Teaching Assistants (TAs): Maruan Al-Shedivat (GHC 8229), Lisa Lee (GHC 8011), Xun Zheng (GHC 8013), Hao Zhang (NSH 4225), Paul Liang (GHC 8011). More details of office hours of TAs can be found at https://sailinglab.github.io/pgm-spring-2019/ .

  3.4. Students can contact the course instructors at 10708-instructor@cs.cmu.edu , or use Piazza for their questions.


4. Grading
Grading in this course is split between a variety of tasks. The course has 4 homework assignment which account for 40 \% of grades. The final project is worth 46 \% of all points in the course. Each student also has to scribe 1-2 lectures in the course; this accounts for another 10 \% of the course grade. Participation in discussions on Piazza, completion of mid-semester evaluation and contribution to actions that improve the class are worth 3.5 \%, 0.5 \% and 1 \% of the course grade. There are no exams in this class.

One can visit https://sailinglab.github.io/pgm-spring-2019/logistics/#grading to know more about grading and policies for late submissions.

5. Projects
Students are expected to form groups of 3-4 for projects in this course. The project will have four main parts, viz. proposal, a midway report, a final report, and a poster (or oral) presentation. The projects would be expected to be based on research, and write-ups would be expected to be similar to conference style papers.

*** ––>

## The Fundamental Questions of Graphical Modeling

A graphical model is a method of modeling a probability distribution for reasoning under uncertainty, which is needed in applications such as speech recognition and computer vision.
We usually have a sample of datapoints: $$ D = \{X_{1}^{(i)},X_{2}^{(i)},...,X_{m}^{(i)} \}_{i=1}^N $$.
The relations of the components in each $$ X $$ can be depicted using a graph $$ G $$.
We then have our model $$ M_G $$.

<figure id="sample-graph" class="l-gutter">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-01/sample-graph.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong> A graph for a model </strong>
  </figcaption>
</figure>

Graphical models allow us to address three fundamental questions:

<ol>
  <li>How should I represent my data in a way that reflects domain knowledge while acknowledging uncertainty?</li>

  <li>How do I make inferences from this data?</li>

  <li>How can I learn the &quot; right &quot; model for this data?</li>
</ol>

Each of these questions can be rephrased as a question about probability distributions:

<ol>
  <li>What is the joint probability distribution over my input variables? 
Which state configurations of the distribution are actually relevant to the problem?</li>

  <li>How can we obtain the state probabilities? 
Do we use maximum-likelihood estimation, or can we use domain knowledge?</li>

  <li>How can we compute conditional distributions of unobserved (latent) variable without needing to sum over a large number of state configurations?</li>
</ol>

In the next section, we give an example to show how graphical models provide an effective way of answering these questions. 

***

## Structure within a Cell: real world example of graphical models with structure among the RVs

<figure id="cell-ex-figure" class="l-body-outset">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-01/cell-ex.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong> Cell Structure Example </strong>
  </figcaption>
</figure>

Receptors: receives signal from cell surface
Kinase: Reads and decodes the signal
TF: Takes in the signal and triggers production of dna with dna template
Gene: DNA templates

We can incoporate such domain knowledge to impose structure on the RVs $$ X_1,..,X_8 $$.
A preliminary way is to partition the RV's into compartments they reside in within a cell.
Then we can model edges(pathway) that model the dependencies(communication) among the RVs(nodes).


With this structure, we can better express the joint probabilities among the RVs than with a full joint distribution table. The Factorization Law gives us a way to do so. 
The Factorization Law is a graph traversal algorithm that outputs a unique representation of the joint probability of the RVs. Concisely, we traverse the graph and identify the conditional probabilities of each node given its parent nodes, and marginal probabilities of nodes that do not have parents, then multiply all terms together for the joint probability of all nodes.

There are 3 main benefits of representing the joint distribution in this manner (with a graph structure and conditional probabilities that tie parent nodes and child nodes)

***

## 3 Main Benefits of Graphical Models

1. Save cost in representing the joint distribution
By modeling the dependencies among the RVs with a graph and conditionals, the number of parameters needed to decribe the joint distribution is much fewer than when using a full joint distribution table.

Assume all RVs are binary.

$$ P(X_1) \rightarrow 1 $$ parameter 
$$ P(X_2) \rightarrow 1 $$ parameter 
$$ P(X_3|X_1) \rightarrow 2 $$ parameters 
$$ P(X_4|X_2) \rightarrow 2 $$ parameters
$$ P(X_5|X_2) \rightarrow 2 $$ parameters
$$ P(X_6|X_3, X_4) \rightarrow 4 $$ parameters 
$$ P(X_7|X_6) \rightarrow 2 $$ parameters 
$$ P(X_8|X_5, X_6) \rightarrow 4 $$ parameters

Total = 18 parameters

Meanwhile, with a full joint distribution table, we would need $$ 2^{8}-1 $$ parameters.

2. Data integration

By factoring the joint distribution into modular terms, each term becomes self-contained and we can estimate each term with only the relevant datapoints (e.g. to estimate $$ P(X_8|X_5, X_6) $$ we only need data for $$ X_8, X_5 and X_6 $$)
Therefore, the problem of joint distribution estimcation can be modularized into smaller pieces and integrated later by multiplication.

Examples:
<dl>
    <dt>Cell molecules example</dt> 
    <dd>- one lab can study the subtree formed by $$ X_1, X_3, X_6, X_7, X_8 $$ and another lab can study $$ X_2, X_4, X_5 $$, then fuse their estimations together by multiplying the terms by their dependencies.</dd>
    <dt>Cellphone usage</dt>
    <dd>- we can separately study the distribution represented by the user's text, image and network data and fuse them toegether with a graphic model to derive the joint distribution.</dd>
    <dt>Genomics/biology</dt>
    <dd>- we routinely combine various data together with graphic models.</dd>
</dl>

3. Graphical models provide a generic method of representing knowledge and making inferences

We can encode our domain knowledge through priors and incorporate them into our inferece via the Bayes Theorem:
$$ p(h|d) = \frac{p(d|h)p(h)}{p(d)} $$
$$ p(h|d) $$=posterior
$$ p(d|h) $$=likelihood
$$ p(h) $$=prior

A graphical model provides a structured and efficient way for doing these computations.
Therefore, a graphical model along with the Bayes Theorem provide a universal way of representing knowledge and computation.

***

## PGM's vs GM's

Next, we will elaborate on the difference between Probabilistic Graphical Models (PGM) and Graphical Models (GM).
In brief, a PGM adds structure to a multivariate statistical distribution, while a GM adds structure to any multivariate objective function.

A PGM minimizes the cost of designing a probability distribution. 
Formally, a PGM is a family of distributions over a given set of random variables.
These distributions must be compatible with all the independence relationships among the variables, which are encoded in a graph.

In the graph itself, the type of edge used denotes the relationship among the variables.
Directed edges denote causality, while undirected edges denote correlation.

<figure id="graph-families" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-01/dag.png' | relative_url }}" />
      <figcaption>
        <strong>Bayesian network</strong>
        A directed acyclic graph (DAG).
      </figcaption>
    </div>
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-01/undirected.png' | relative_url }}" />
      <figcaption>
        <strong>Markov random field</strong>
        An undirected graph.
      </figcaption>
    </div>
  </div>
</figure>

For instance, the Bayes net uses a directed acyclic graph (DAG).
Each node in a Bayes net has a Markov blanket, composed of its parents, its children, and its children's parents.
Every node is conditionally independent of the nodes outside its Markov Blanket.
Therefore, the local conditional probabilities as well as the graph structure completely determine the joint probability distribution.
This model can be used to generate new data.

By contrast, the Markov Random Field uses an undirected graph.
Every node is conditionally independent of the other graph nodes, except for its immediate neighbors.
To determine the joint probability distribution, we need to know local contingency functions as well as structural cliques.
This model cannot explicitly generate new data.

***

## Equivalence Theorem / PGM Genealogy

We will be discussing the Equivalence Theorem, stated as follows:

For a graph G, 

Let $D_1$ denote the family of all distributions that satisfy I(G), 

Let $D_2$ denote the family of all distributions that factor according to G, 

then $$D_1 \equiv D_2$$.

The theorem is interpreted in two ways:
<ol>
  <il>Separation properties in the graph imply independence properties about the associated variables.</il>

  <il>For the graph to be useful, any conditional independence properties we can derive from the graph should hold for the probability distribution that the graph represents.</il>

</ol>

The study of Graphical Models involves the following parts:
<ol>
  <il>Density estimation with parametric and nonparametric methods</il>

  <il>Regression : Linear, conditional mixture, nonparametric</il>

  <il>Classification with generative and discriminative approaches</il>

  <il>Clustering</il>
</ol>

An example genealogy of graphical models is as follows: 

<figure id="gmm" class="l-body-outset">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-01/gmm.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong> GMM </strong>
  </figcaption>
</figure>

***

## Fancier GMs and Applications of GMs

GMs can be applied in numerous more advanced ways to solve complex problems in areas like reinforcement learning, machine translation, genetic pedigrees and solid state physics.

The applications of GMs include but are not limited to the following areas: Machine Learning, Computational Statistics, Computer Vision and Graphics, Natural Language Processing, Informational Retrieval, Robotic Control, etc. 

## Why should we study graphical models?

Design and analysis of algorithms in the fields of artificial intelligence, machine learning, natural language processing, etc. encounter issues like uncertainty and complexity. In graphical models, we use the idea of modularity, and view such complex problems as combinations of simpler parts. Tools from graphical models can be used for communication of information in networks. They can also be used to ease computation (simplify computational complexities and reduce time required for computations). As such, graphical model formalism can be used for development of efficient software packages for decision making and learning in problems rely on huge datasets.

Below we mention a few prominent reasons why one can use probabilistic graphical models:
<ul style="list-style-type:square">
  <li>In graphical models, we break tasks into combinations of simpler parts. Probability theory helps to connect these simple parts with each other in a coherent and consistent manner.</li>

  <li>Graph theory gives an easy-to-understand interface in which models with multiple variables can be cast. Such interfaces help to uncover interactions, dependencies between difference sets of variables. As a consequence, graph theory also helps in the design of more efficient algorithms.</li>

  <li>Formalisms in general graphical model can be used for tasks in a plethora of fields like information theory, cyber security, systems engineering, pattern recognition etc.</li>

  <li>The generalizability of graphical model framework gives us a way to view different systems as occurences of a common underlying formalism.</li>
</ul>

## Plans for class

In this course, we will see an in-depth exploration of issues related to learning within the probabilistic graphical model formalism. The course will be divided into three main sections: Fundamentals of graphical models, advanced topics in graphical models, popular graphical models and applications. An outline of the topics that will be covered in this class is given below:
<ul>
  <li>Fundamentals of graphical models
  <ul>
    <li>Bayesian Network and Markov Random Fields</li>

    <li>Discrete, Continuous and Hybrid models, Exponential family, Generalized Linear Models</li>

    <li>Inference and Learning</li>
  </ul>
  </li>


  <li>Advanced topics and latest developments in graphical models
  <ul>
    <li>Approximate inference</li>

    <li>Infinite graphical models: nonparametric Bayesian models</li>

    <li>Optimization-theoretic formulations for graphical models, e.g., Structured sparsity</li>

    <li>Graphical models vs Deep nets</li>

    <li>Nonparametric and spectral graphical models</li>

    <li>Alternative graphical model learning paradigms</li>
  </ul>
  </li>


  <li>Popular graphical models and applications
  <ul>
    <li>Multivariate gaussian models</li>

    <li>Conditional random fields</li>

    <li>Mixed-membership</li>
  </ul>
  </li>    
<ul>

